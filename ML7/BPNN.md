
## 七、BP神经网络

## 1. 题目一：采用 scikit-learn 中的 MLPClassifier 对红酒数据集进行分类，并通过特征和边界的可视化，直观体会多层感知机网络中的隐层上神经元数量、隐层层数、激活函数、正则化项系数等超参数对模型复杂程度的影响。

### 具体内容：

1. **选取前两个特征，建立多层感知机网络进行多分类。**

2. **可视化**  
   - 通过散点图可视化数据样本（之前选择的两个特征），并画出模型训练后得到的决策边界。

#### 讨论：

- **讨论一：改变单隐层中神经元个数（如 10 个，100 个），其他参数不变，观察其对决策边界的影响。**
  
- **讨论二：改变神经网络深度（如深度为 2，每层 10 个神经元），其他参数不变，与讨论一进行对比，观察神经网络深度对决策边界的影响。**

- **讨论三：在讨论一（或讨论二）的基础上，改变激活函数（如 `tanh`、`relu`），与讨论一（或讨论二）进行对比，观察不同激活函数对决策边界的影响。**

- **讨论四：在讨论三的基础上，增大正则化系数，观察正则化对决策边界的影响。**

#### 总结：

- 综合上述讨论，隐层上神经元数量、隐层层数、激活函数、正则化项系数对模型复杂程度有何影响。

---

## 2. 题目二：采用 scikit-learn 中的 MLPClassifier 对自带手写数字数据集进行分类。

### 具体要求：

1. **导入数据集**  
   - 手写数字集是 sklearn 中自带的数据集，它是一个三维数组 `(1797, 8, 8)`，即有 1797 个手写数字，每个数字由 8×8 的像素矩阵组成。矩阵中每个元素都是 0-16 范围内的整数。分类标签为 0-9 的数字。

2. **模型建立**  
   - 使用 `MLPClassifier` 建立分类模型。

3. **输出**  
   - 输出分类结果的准确率。

#### 讨论：

- **讨论五：结合模型复杂度与模型泛化误差之间的关系，调节模型超参数，提升模型泛化性能。**  
   可尝试调节隐层神经元个数和隐层数、激活函数、学习率、正则项系数等超参数。

---

## 3. 题目三：编写 BPNN 算法，对 iris 数据集/手写数字集进行二分类或多分类。

### 具体要求：

1. **数据样本标签处理**  
   - 二分类任务：正类为 1，负类为 0。  
   - 多分类任务：将样本标签变为 one-hot 向量。

2. **搭建浅层神经网络**  
   - 隐层数 1-2 个即可。每层神经元的个数自选。

3. **激活函数**  
   - 自选（`relu`、`sigmoid`、`tanh`）。  
   - 代价函数：自选（交叉熵损失、均方误差）。

4. **输出**  
   - 输出分类准确率。

5. **可视化**  
   - 迭代的代价函数曲线。

6. **尝试手写 BP 网络链式法则的反向传播计算过程**。

#### 注意事项：

- **提高运算效率**  
   - 算法编写尽量使用向量化技术，避免使用 `for` 循环遍历样本和神经元。

- **权重和偏置计算**  
   - 一般将权重 `w` 和偏置 `b` 分开进行计算。

- **初始化权重**  
   - 为避免梯度消失或梯度爆炸，通常采用随机初始化权重 `w`，而不是将权重全部初始化为 0 或 1。  
     示例：若要生成服从 `𝒩(0, √(2/(𝑛𝑖𝑛+𝑛𝑜𝑢𝑡)))` 分布的随机数，可使用如下程序：  
     ```python
     np.random.randn(m, n) * np.sqrt(2 / (nin + nout))
     ```
     其中 `nin` 为神经元的输入连接数量，`nout` 为神经元的输出连接数量，`m` 和 `n` 分别为返回数组的行数和列数。

- **注意数组维度**  
   - 向量计算过程中，要注意数组维度，避免出现异常 bug。避免使用一维数组（例如 `np.random.randn(5)`），这不是列向量也不是行向量！  
     可通过下面示例 `np.random.randn(5, 1)` 或 `np.random.randn(1, 5)` 创建列向量或行向量，或使用 `reshape` 方法变换数组的维度。
