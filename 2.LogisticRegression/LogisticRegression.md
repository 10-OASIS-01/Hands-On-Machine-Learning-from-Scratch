
## 二、逻辑回归

### [1. 采用 scikit-learn 中的 LogisticRegression 逻辑回归模型对 iris 数据集进行二分类。](ML2_1.ipynb)


#### 具体内容：

1. **特征可视化**  
   - 任选两个特征和两种类别进行散点图可视化，观察是否线性可分。

2. **模型建立**  
   - 使用选取的特征和两种类别建立二分类模型。

3. **输出**  
   - 输出决策函数的参数、预测值、分类准确率等。

4. **决策边界可视化**  
   - 将二分类问题的边界可视化。

### [2. 采用 scikit-learn 中的 LogisticRegression 逻辑回归模型对 iris 数据集进行多分类。](ML2_2.ipynb)

#### 具体内容：

1. **模型建立**  
   - 任选两个特征和全部类别进行散点图可视化，并建立多分类模型。

2. **输出**  
   - 输出决策函数的参数、预测值、分类准确率等。

3. **决策边界可视化**  
   - 将多分类问题的边界可视化。  
   提示：可以使用 `numpy` 中的 `meshgrid` 生成绘图网格数据，使用 `matplotlib` 中的 `contourf` 将等高线之间颜色进行填充。

#### 讨论：

- **讨论一：不同多分类策略的效果如何？有何差异？**  
   1. 尝试对比 `LogisticRegression` 中的 `multi_class='ovr'` 或 `'multinomial'` 两种多分类的差异。  
   2. 尝试使用 Multiclass classification 中提供的 3 种多分类策略，并对比效果。  
   提示：进行对比时，要保证数据集划分一致且分析的特征一致。可从训练集、测试集准确率和边界可视化角度进行对比。

### [3. 采用 scikit-learn 中的 LogisticRegression 逻辑回归模型对非线性数据集进行分类。](ML2_3.ipynb)

#### 具体内容：

1. **数据集**  
   - 使用 sklearn 自带数据生成器 `make_moons` 产生两类数据样本，示例程序如下，参数可自行修改。

2. **特征衍生（数据增强）**  
   - 使用 sklearn 自带的 `sklearn.preprocessing.PolynomialFeatures` 生成指定阶次的多项式特征，从而得到所有多项式组合成的新特征矩阵，`degree` 参数任选。

3. **模型建立**  
   - 在新特征基础上建立逻辑回归二分类模型。

4. **决策边界可视化**  
   - 绘制决策边界，观察非线性边界的变化。

#### 讨论：

- **讨论二：在不加正则项的情况下，改变特征衍生的特征数量（即 `degree` 参数），观察决策边界的变化情况，以及训练集和测试集分数，体会模型从欠拟合 -> 拟合 -> 过拟合的过程。**  
   提示：可使用 `for` 循环对不同 `degree` 进行遍历，观察模型的建模结果。可通过绘制训练集和测试集分数曲线帮助观察（如示例图）。

- **讨论三：在讨论二的基础上选择一种模型过拟合的 `degree`，在模型中分别加入 'l1' 和 'l2' 正则项，观察决策边界的变化情况，以及训练集和测试集分数，体会两种正则项对模型的作用。**

- **讨论四：可尝试手动调整 `degree`、正则项系数 `C` 和正则项种类，寻找使模型泛化性能最好的一组参数。**  
   提示：手动调参采用“单一变量”原则。可先设定正则项种类（如 'l1'）和正则项系数 `C`（如默认），再人为设定特征最高阶次 `degree` 的范围进行 `degree` 寻优，在选定的 `degree` 和 'l1' 正则化后，设定正则项系数 `C` 的范围进行寻优。

### [4. 使用 numpy 编写逻辑回归算法，对 iris 数据进行二分类。](ML2_4.ipynb)

`LogisticRegression.py `中为逻辑回归算法的具体实现代码 

#### 具体内容：

1. **任选两个特征和两个类别进行二分类。**

2. **输出**  
   - 输出决策函数的参数、预测值、分类准确率等。

3. **可视化**  
   - 选取两个特征进行散点图可视化，并可视化决策边界。

### [5. 使用 numpy 编写逻辑回归算法，对 iris 数据进行多分类。](ML2_5.ipynb)

#### 具体内容：

- 输出决策函数的参数、预测值、分类准确率等。

#### 提示：

1. 可采用 OVR、OVO、ECOC 策略。
2. 可采用 CrossEntropy Loss +

####  softmax 策略：
 
   a) 需将三个类别（如 0, 1, 2）进行 one-hot 编码。  
   b) 每个线性分类器对应一组模型参数，3 个线性分类器对应 3 组模型参数。  
   c) 可通过 softmax 回归计算多种类别的概率（K 种类别概率和为 1）。  
   d) 通过最小化 CrossEntropy Loss 的梯度下降算法进行分类器参数寻优。
