# Hands-on Learning of Machine Learning from Scratch

### [I. Linear Regression](1.LinearRegression/LinearRegression.md)

#### 1. Use the LinearRegression (Least Squares) model from scikit-learn to predict the Boston Housing dataset, and build models using both the normal equation and stochastic gradient descent methods.

#### 2. Implement the linear regression algorithm from scratch using numpy, optimizing the model using gradient descent (BGD) to predict the Boston Housing prices.

### [II. Logistic Regression](2.LogisticRegression/LogisticRegression.md)

#### 1. Use the LogisticRegression model from scikit-learn to perform binary classification on the Iris dataset.

#### 2. Use the LogisticRegression model from scikit-learn to perform multi-class classification on the Iris dataset.

#### 3. Use the LogisticRegression model from scikit-learn to classify a non-linear dataset.

#### 4. Implement the logistic regression algorithm from scratch using numpy for binary classification on the Iris dataset.

#### 5. Implement the logistic regression algorithm from scratch using numpy for multi-class classification on the Iris dataset.

### [III. Support Vector Machines](3.SupportVectorMachine/SupportVectorMachine.md)

#### 1. Use the linear SVM from scikit-learn for binary classification on the Iris dataset.

#### 2. Use different SVM kernel functions to perform binary classification on various datasets.

#### 3. Use the SVM classifier from scikit-learn to classify the Breast Cancer Wisconsin dataset.

#### 4. Implement the SMO algorithm from scratch using numpy to create a linear SVM classifier, and perform binary classification on the Iris dataset.

### [IV. Decision Trees](4.DecisionTree/DecisionTree.md)

#### 1. Use the DecisionTreeClassifier from scikit-learn to predict the Wine dataset.

#### 2. Use the DecisionTreeClassifier from scikit-learn to predict the KDD Cup 99 dataset.

#### 3. Implement the CART classification/regression tree algorithm from scratch using numpy, and perform predictions on the Iris dataset or the California Housing dataset.

### [V. Random Forests](5.RandomForest/RandomForest.md)

#### 1. Use the RandomForestRegressor from scikit-learn to predict the California Housing dataset.

#### 2. Implement the Random Forest algorithm from scratch using numpy, and predict either the Wine dataset or the California Housing dataset (choose one), comparing the model's score with the results from scikit-learn's built-in evaluator.

### [VI. AdaBoost](6.AdaBoost/AdaBoost.md)

#### 1. Use the AdaBoostClassifier from scikit-learn to predict the Wine dataset.

#### 2. Implement the AdaBoost-SAMME algorithm from scratch using numpy, and predict the Breast Cancer dataset, showing the model's score.

#### 3. Use the GradientBoostingRegressor from scikit-learn to predict the California Housing dataset.

### [VII. BP Neural Networks](7.BackpropagationNeuralNetwork/BackpropagationNeuralNetwork.md)

#### 1. Use the MLPClassifier from scikit-learn to classify the Red Wine dataset, and visualize the feature boundaries. Intuitively understand the effect of hyperparameters such as the number of neurons in hidden layers, the number of hidden layers, activation functions, and regularization coefficients on model complexity.

#### 2. Use the MLPClassifier from scikit-learn to classify the built-in handwritten digit dataset.

#### 3. Implement the BPNN algorithm from scratch using numpy, performing binary or multi-class classification on the Iris dataset or the handwritten digits dataset.
